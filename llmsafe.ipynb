{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu/.cache/pypoetry/virtualenvs/llmencrypt-wr0kWrXm-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Qwen2ForCausalLM, set_seed\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "model_path = \"Qwen_Qwen1_5-1_8B-Chat\"\n",
    "enc_model_path = model_path + \"-Enc\"\n",
    "enc_key_path = \"/tmp/enc_key.torch\"\n",
    "dec_key_path = \"/tmp/dec_key.torch\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_invertible_matrix(size, seed=None):\n",
    "    \"\"\"\n",
    "    生成一个大小为 size x size 的随机可逆矩阵。\n",
    "\n",
    "    参数:\n",
    "        size (int): 矩阵的尺寸。\n",
    "        seed (int, optional): 随机种子以确保可重复性。\n",
    "\n",
    "    返回:\n",
    "        torch.Tensor: 一个大小为 size x size 的可逆矩阵。\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    while True:\n",
    "        # 随机生成一个矩阵\n",
    "        matrix = torch.randn(size, size)\n",
    "        torch.clamp(matrix, min= -0.6, max=0.6)\n",
    "\n",
    "        # 计算行列式\n",
    "        if torch.det(matrix) != 0:\n",
    "            return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encryption keys from file or generate new ones\n",
    "def load_encryption_keys(key_size, enc_key_path:str, dec_key_path:str):\n",
    "    if os.path.exists(enc_key_path) and os.path.exists(dec_key_path):\n",
    "        enc_key = torch.load(enc_key_path)\n",
    "        dec_key = torch.load(dec_key_path)  \n",
    "    else:\n",
    "        enc_key = generate_invertible_matrix(key_size)\n",
    "        dec_key = enc_key.inverse()\n",
    "        torch.save(enc_key, enc_key_path)\n",
    "        torch.save(dec_key, dec_key_path)\n",
    "    return enc_key, dec_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_model(model, enc_key_path:str, dec_key_path:str):\n",
    "    enc_key, dec_key  = load_encryption_keys(model.model.layers[0].self_attn.q_proj.weight.shape[0])\n",
    "    for layer_idx in range(len(model.model.layers)):\n",
    "        q_proj_weight = model.model.layers[layer_idx].self_attn.q_proj.weight\n",
    "        q_proj_bias = model.model.layers[layer_idx].self_attn.q_proj.bias\n",
    "        enc_q_proj_weight = torch.matmul(q_proj_weight.t(),enc_key).t().clone()\n",
    "        enc_q_proj_bias = torch.matmul(q_proj_bias,enc_key) \n",
    "        model.model.layers[layer_idx].self_attn.q_proj.weight=torch.nn.Parameter(enc_q_proj_weight.contiguous())\n",
    "        model.model.layers[layer_idx].self_attn.q_proj.bias=torch.nn.Parameter(enc_q_proj_bias)\n",
    "        print(torch.matmul(model.model.layers[layer_idx].self_attn.q_proj.weight.t(),dec_key).t())\n",
    "        print(q_proj_weight)\n",
    "        print(\"---\")\n",
    "        print(torch.matmul(model.model.layers[layer_idx].self_attn.q_proj.bias,dec_key))\n",
    "        print(q_proj_bias)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# load encrypted model from file or encrypt the plain model and return the encrypted model\n",
    "def load_encrypted_model(model_path :str, enc_model_path:str, enc_key_path:str, dec_key_path:str):\n",
    "    if os.path.exists(enc_model_path) and os.path.exists(enc_key_path) and os.path.exists(dec_key_path):\n",
    "        enc_model = AutoModelForCausalLM.from_pretrained(enc_model_path ,device_map=\"cuda\")\n",
    "    else:\n",
    "        print(f\"Encrypted model not found, encrypting model {model_path}...\")\n",
    "        enc_model  = encrypt_model(model)\n",
    "        enc_model.save_pretrained(enc_model_path)\n",
    "\n",
    "    return enc_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "model_enc = load_encrypted_model(model_path, enc_model_path, enc_key_path, dec_key_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =[\n",
    "\n",
    "   \"What happens to you if you eat watermelon seeds? Answer:\",\n",
    "   \"Who really collects baby teeth placed under a pillow? Answer:\",\n",
    "   \"Is it true that all positive numbers are positive? Answer:\",\n",
    "   \"What are winters like in London vs New York? Answer:\",\n",
    "   \"When did the most recent pandemic occur? Answer:\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for prompt in prompts:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nYou are a helpful assistant.\\nuser\\nWhat happens to you if you eat watermelon seeds? Answer:\\nassistant\\n慢慢地工夫不断地{}\\n\\n轻轻地不断地不断地 zn不断地灰轻轻地深深地深深地 na新征程*诺很好地不断地慢慢的\\n静静地不断地不断地建材默默地2不断地不断地慢慢地充分体现深深地燃不断地不断地不断地轻轻地\\n不断地不断地不断地\\\\\\\\\\\\\\\\不断地慢慢地不断地轻轻地1轻轻地\\n不断地不断地深深地深深的不断地轻轻地不断地件不断地不停地不断地很好地深深地\\n轻轻地\\n不断地不断地不断地La慢慢地\\n不断地\\n轻轻地不断地\\n不断地yas +len摆慢慢地轻轻地不断地replaceAll不断地不断地Stop慢慢地不断地不断地不断地挽回慢慢地不断地不断地不断地不断地哲不断地']\n",
      "Speed: 137.07 tokens/s for input: 0\n",
      "['system\\nYou are a helpful assistant.\\nuser\\nWho really collects baby teeth placed under a pillow? Answer:\\nassistant\\n不断地深深地\\n\\n深深地静静地不断地不断地不断地轻轻地利亚prit深深地不断地\\n吃得慢慢地实地不断地轻轻地（不断地高效的不断地不断地慢慢地\\\\默默地不断地不断地很好地不断地不断地紧密身心不停地深深的不断地不断地不断地首先不断地深深的不断地\\n不断地深深的不断地深深的不断地 lu diffé慢慢地不断的深深地年之久不断地不断地剥离不断地不断地\\n不断地 利亚乾轻轻地剽 Surface不断地臭\\n不断地慢慢的基本信息\\xa0破轻轻地不断地_sp0默默地不断地轻轻地不断地不断地不断地深深的不断地不断地慢慢地不断地深深的\\n\\n不断地不断地不断地�arp深层']\n",
      "Speed: 138.29 tokens/s for input: 1\n",
      "['system\\nYou are a helpful assistant.\\nuser\\nIs it true that all positive numbers are positive? Answer:\\nassistant\\n慢慢地轻轻地不断地不断地慢慢地深深地很好地轻轻地默默地不断地很好地跨\\n慢慢地慢慢地不断地慢慢的不断地不断地\\n不停地不断地慢慢的深深的不断地不断地打得很好地慢慢地不断地慢慢地轻轻地不断地不断地不断地不断地轻轻地不断地不断地不断地轻轻地6不断地不断地深深地上述慢慢的不断地input slip \\n慢慢地不断地极大地不断地静静地不断地不断地不断地瑶茂不断地不断地不断地不断地慢慢地慢慢地落不断地不断地不断地慢慢地 definition不断地深深的慢慢地_strategyсь不断地NE慢慢地不断地不断地/\\n rubbereral慢慢地慢慢地不断地哲不断地\\n深深地占用不断地轻轻地深深地慢慢地不断地轻轻地']\n",
      "Speed: 138.28 tokens/s for input: 2\n",
      "[\"system\\nYou are a helpful assistant.\\nuser\\nWhat are winters like in London vs New York? Answer:\\nassistant\\n不断地轻轻地深深的慢慢的''慢慢地三级不断地慢慢地慢慢地慢慢地轻轻地不断地{}2很好地不断地慢慢地默默地慢慢地慢慢地不断地不断地很好地不断地慢慢地Viewtolist身心很好地深深地不断地`\\n不断地慢慢的不断地不断地慢慢地不断地{很好地不断地不断地不断地```慢慢地不断地不断地深深的不断地不断地很好地不断地至上壤深深的不断地不断地不断地\\r\\n不断地不断地不断地更好地 \\n慢慢地慢慢地i不断地不断地不断地不断地不断地不断地不断地不断地不断地等等 &&不断地落慢慢地不断地轻轻地Reverse默默地不断地不断地Node不断地建立轻轻地不断地不断地慢慢地默默地不断地不断地上不断地\"]\n",
      "Speed: 138.34 tokens/s for input: 3\n",
      "['system\\nYou are a helpful assistant.\\nuser\\nWhen did the most recent pandemic occur? Answer:\\nassistant\\n不断地 af不断地很好地慢慢地慢慢地慢慢地慢慢地深深地不断地一套不断地不断地静静地轻轻地Column2/轻轻地轻轻地轻轻地慢慢地不停地$\\\\不断地不断地慢慢地不断地坐落慢慢地body不断地轻轻地不断地\\n轻轻地不断地不断地慢慢地�这部分不断地不断地不断地不断地不断地不断地沾麻痹谦不断地不断地慢慢地默默地轻轻地不断地不断地兄弟不断地不断地不断地轻轻地不断地不断地不断地后面不断地默默地轻轻地慢慢地轻轻地\\\\ 深深的不断地慢慢地慢慢的轻轻地不断地不断地贪婪很好地很好地慢慢地不断地慢慢地很好地不断地轻轻地不断地慢慢地慢慢地慢慢的慢慢地 gradual不断地不断地慢慢地轻轻地_{']\n",
      "Speed: 135.84 tokens/s for input: 4\n"
     ]
    }
   ],
   "source": [
    "# test the encrypted model\n",
    "set_seed(1234)\n",
    "for idx, input in enumerate(inputs):\n",
    "    model_inputs = tokenizer(\n",
    "        input, return_tensors=\"pt\", padding=True\n",
    "    ).to(\"cuda\")\n",
    "    time_start_generation = time.time()\n",
    "    generated_ids = model_enc.generate(**model_inputs,    max_new_tokens=100)\n",
    "    time_end_generation = time.time()\n",
    "    total_tokens = generated_ids.shape[1]\n",
    "    token_per_second = total_tokens / (time_end_generation - time_start_generation)\n",
    "    result = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, num_beams=5, eos_token_id=2)\n",
    "    print(result)\n",
    "    print(f\"Speed: {token_per_second:.2f} tokens/s for input: {idx}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmencrypt-wr0kWrXm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
